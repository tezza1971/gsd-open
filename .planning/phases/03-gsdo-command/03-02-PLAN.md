---
phase: 03-gsdo-command
plan: 02
type: execute
wave: 2
depends_on: [03-01]
files_modified:
  - src/lib/enhancer/llm-client.ts
  - src/lib/enhancer/llm-client.test.ts
  - src/lib/enhancer/enhancer.ts
  - src/lib/enhancer/enhancer.test.ts
autonomous: true

must_haves:
  truths:
    - "LLM client can detect and use OpenCode's configured model"
    - "Enhancement prompts include all required context (command, install.log, docs, GSD source)"
    - "Enhancer analyzes each command individually with focused prompts"
    - "Enhancer applies conservative fixes (naming, references, parameters, templates)"
    - "Enhancement failures for individual commands don't stop the batch"
  artifacts:
    - path: "src/lib/enhancer/llm-client.ts"
      provides: "LLM integration using OpenCode's configured model"
      exports: ["callLLM", "detectOpenCodeModel"]
      min_lines: 80
    - path: "src/lib/enhancer/enhancer.ts"
      provides: "Per-command enhancement logic with prompt building"
      exports: ["enhanceCommand", "enhanceAllCommands"]
      min_lines: 120
    - path: "src/lib/enhancer/llm-client.test.ts"
      provides: "LLM client test suite"
      min_lines: 40
    - path: "src/lib/enhancer/enhancer.test.ts"
      provides: "Enhancement logic test suite"
      min_lines: 60
  key_links:
    - from: "src/lib/enhancer/enhancer.ts"
      to: "src/lib/enhancer/llm-client.ts"
      via: "callLLM function"
      pattern: "callLLM\\("
    - from: "src/lib/enhancer/llm-client.ts"
      to: "OpenCode config"
      via: "fs.readFile for settings.json or similar"
      pattern: "readFile.*settings"
    - from: "src/lib/enhancer/enhancer.ts"
      to: "GSD source markdown"
      via: "fs.readFile"
      pattern: "readFile.*\\.md"
---

<objective>
Implement LLM-powered enhancement logic that analyzes each transpiled command individually, building focused prompts with full context (command, install.log, docs, GSD source) and applying conservative fixes.

Purpose: Transforms algorithmically-transpiled commands into production-ready OpenCode commands using intelligent LLM analysis.
Output: Working enhancement system that uses OpenCode's LLM to improve command quality autonomously.
</objective>

<execution_context>
@C:\Users\Terence\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\Terence\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@C:\Users\Terence\code\gsd-open\.planning\PROJECT.md
@C:\Users\Terence\code\gsd-open\.planning\ROADMAP.md
@C:\Users\Terence\code\gsd-open\.planning\STATE.md
@C:\Users\Terence\code\gsd-open\.planning\phases\03-gsdo-command\03-gsdo-command-CONTEXT.md

# Prior plan in this phase
@C:\Users\Terence\code\gsd-open\.planning\phases\03-gsdo-command\03-01-PLAN.md

# Existing code
@C:\Users\Terence\code\gsd-open\src\lib\enhancer\engine.ts
@C:\Users\Terence\code\gsd-open\src\lib\enhancer\types.ts
@C:\Users\Terence\code\gsd-open\src\lib\transpiler\types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM client with OpenCode model detection</name>
  <files>
    src/lib/enhancer/llm-client.ts
    src/lib/enhancer/llm-client.test.ts
  </files>
  <action>
Create LLM integration layer that uses OpenCode's configured model.

**llm-client.ts** - Implement LLM interaction:

1. `detectOpenCodeModel(opencodeConfigPath: string): { provider: string; model: string } | null`
   - Read OpenCode's configuration file (check for settings.json, config.json, or similar)
   - Parse LLM configuration (provider like "anthropic", "openai", model name)
   - Return detected configuration or null if not found
   - Handle missing/malformed config gracefully

2. `callLLM(prompt: string, opencodeConfigPath: string): Promise<string>`
   - Detect OpenCode's configured model (use detectOpenCodeModel)
   - If no model configured, throw error with actionable message
   - Call LLM using detected provider/model (use environment variables for API keys)
   - For Anthropic: use fetch to call Messages API directly (no SDK - zero external dependencies)
   - For OpenAI: similar direct API approach
   - Return LLM response text
   - Handle API errors with retry logic (1 retry with exponential backoff)
   - Throw on persistent failures

**llm-client.test.ts** - Test suite:
1. Mock detectOpenCodeModel with various configs
2. Mock fetch for API calls
3. Test retry logic on failures
4. Test error messages when model not configured

Use Node.js built-in fetch (available in Node 20+).
No external HTTP client dependencies.
  </action>
  <verify>
npm test -- llm-client.test.ts shows model detection and API calling work correctly with mocked responses
  </verify>
  <done>
LLM client can detect OpenCode's model configuration and make API calls with retry logic, test suite validates both happy and error paths
  </done>
</task>

<task type="auto">
  <name>Task 2: Create per-command enhancement logic</name>
  <files>
    src/lib/enhancer/enhancer.ts
    src/lib/enhancer/enhancer.test.ts
  </files>
  <action>
Implement enhancement logic that analyzes commands individually with focused prompts.

**enhancer.ts** - Implement enhancement functions:

1. `enhanceCommand(command: OpenCodeCommand, context: EnhancementContext, opencodeConfigPath: string): Promise<EnhancementResult>`
   - Load original GSD markdown from gsdSkillsPath (derive filename from command name: gsd-plan-phase -> /gsd:plan-phase.md)
   - Build focused prompt including:
     - Current command definition (JSON)
     - Original GSD markdown content
     - Relevant excerpts from install.log (warnings/errors for this command)
     - Relevant excerpts from OpenCode docs (command schema, best practices)
     - Instructions for conservative enhancement (fix naming, broken refs, add missing params, improve templates)
     - Explicit instruction: "Return ONLY the enhanced JSON command object, no explanation"
   - Call LLM with prompt (use callLLM from llm-client)
   - Parse LLM response (extract JSON, handle markdown code fences)
   - Compare original vs enhanced to identify changes
   - Return EnhancementResult with changes list
   - On LLM failure: retry once with refined prompt, then return error result (don't throw)

2. `enhanceAllCommands(context: EnhancementContext, opencodeConfigPath: string): Promise<EnhancementResult[]>`
   - Filter commands to only /gsd-* commands (skip non-GSD commands)
   - Call enhanceCommand for each GSD command sequentially (not parallel - avoid rate limits)
   - Collect all results (successes and failures)
   - Return complete results array

**enhancer.test.ts** - Test suite:
1. Mock callLLM to return enhanced command JSON
2. Mock filesystem for GSD markdown loading
3. Test prompt building includes all context
4. Test JSON parsing from LLM response (with/without code fences)
5. Test graceful failure handling (LLM errors don't stop batch)
6. Test filtering (only enhances gsd-* commands, not others)

Conservative enhancement philosophy:
- Fix broken references to GSD-specific files (STATE.md, ROADMAP.md) by replacing with OpenCode equivalents or removing
- Improve command descriptions (more specific, action-oriented)
- Add missing parameters if obvious from GSD source
- Fix naming issues (inconsistent capitalization, unclear names)
- DO NOT remove, merge, or restructure commands
- DO NOT change core functionality, only improve clarity and compatibility
  </action>
  <verify>
npm test -- enhancer.test.ts shows enhancement logic builds correct prompts, parses LLM responses, and handles failures gracefully
  </verify>
  <done>
Enhancement logic analyzes commands individually, builds context-rich prompts, parses LLM responses, and returns detailed results for each command
  </done>
</task>

</tasks>

<verification>
- [ ] LLM client detects OpenCode's configured model from settings
- [ ] LLM client makes API calls with retry logic
- [ ] Enhancement prompts include all context (command, GSD source, install.log, docs)
- [ ] Enhancer processes each command individually (not batch)
- [ ] Enhancer identifies changes made by LLM
- [ ] Enhancement failures for individual commands don't stop the batch
- [ ] Only gsd-* commands are enhanced (existing OpenCode commands preserved)
- [ ] All tests pass (npm test)
</verification>

<success_criteria>
- LLM integration successfully calls OpenCode's configured model
- Per-command enhancement produces focused, context-rich prompts
- LLM responses are parsed and validated
- Enhancement results track changes made to each command
- Partial success pattern works (some commands enhanced, others skipped on errors)
- Test coverage validates prompt building, LLM interaction, and error handling
</success_criteria>

<output>
After completion, create `.planning/phases/03-gsdo-command/03-02-SUMMARY.md`
</output>
